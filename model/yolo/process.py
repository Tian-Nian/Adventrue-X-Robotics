import cv2
import argparse
import logging 
import numpy as np
import math
from math import sqrt

logging.basicConfig(
    level = logging.INFO,
    format = '[%(name)s] [%(asctime)s.%(msecs)03d] [%(levelname)s] %(message)s',
    datefmt='%H:%M:%S')
logger = logging.getLogger("RDK_YOLO")

coco_names = [
    "person", "bicycle", "car", "motorcycle", "airplane", "bus", "train", "truck", "boat", "traffic light", 
    "fire hydrant", "stop sign", "parking meter", "bench", "bird", "cat", "dog", "horse", "sheep", "cow", 
    "elephant", "bear", "zebra", "giraffe", "backpack", "umbrella", "handbag", "tie", "suitcase", "frisbee", 
    "skis", "snowboard", "sports ball", "kite", "baseball bat", "baseball glove", "skateboard", "surfboard", "tennis racket", "bottle", 
    "wine glass", "cup", "fork", "knife", "spoon", "bowl", "banana", "apple", "sandwich", "orange", 
    "broccoli", "carrot", "hot dog", "pizza", "donut", "cake", "chair", "couch", "potted plant", "bed", 
    "dining table", "toilet", "tv", "laptop", "mouse", "remote", "keyboard", "cell phone", "microwave", "oven", 
    "toaster", "sink", "refrigerator", "book", "clock", "vase", "scissors", "teddy bear", "hair drier", "toothbrush"
]
    
rdk_colors = [
    (56, 56, 255), (151, 157, 255), (31, 112, 255), (29, 178, 255),(49, 210, 207), (10, 249, 72), (23, 204, 146), (134, 219, 61),
    (52, 147, 26), (187, 212, 0), (168, 153, 44), (255, 194, 0),(147, 69, 52), (255, 115, 100), (236, 24, 0), (255, 56, 132),
    (133, 0, 82), (255, 56, 203), (200, 149, 255), (199, 55, 255)]

# COCOå§¿æ€å…³é”®ç‚¹å®šä¹‰
# COCO pose keypoints definition
KEYPOINT_NAMES = [
    "nose", "left_eye", "right_eye", "left_ear", "right_ear",
    "left_shoulder", "right_shoulder", "left_elbow", "right_elbow",
    "left_wrist", "right_wrist", "left_hip", "right_hip",
    "left_knee", "right_knee", "left_ankle", "right_ankle"
]

# å…³é”®ç‚¹è¿æ¥å…³ç³»ç”¨äºç»˜åˆ¶éª¨æ¶
# Keypoint connections for skeleton drawing
SKELETON_CONNECTIONS = [
    (0, 1), (0, 2),  # å¤´éƒ¨ head
    (1, 3), (2, 4),  # çœ¼ç›åˆ°è€³æœµ eyes to ears
    (5, 6),          # è‚©è†€ shoulders
    (5, 7), (7, 9),  # å·¦è‡‚ left arm
    (6, 8), (8, 10), # å³è‡‚ right arm
    (5, 11), (6, 12), # è‚©è†€åˆ°é«‹éƒ¨ shoulders to hips
    (11, 12),        # é«‹éƒ¨ hips
    (11, 13), (13, 15), # å·¦è…¿ left leg
    (12, 14), (14, 16)  # å³è…¿ right leg
]

def get_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--model-path', type=str, default='source/yolo11n_pose_bayese_640x640_nv12.bin', 
                        help="""Path to BPU Quantized *.bin Model.
                                RDK X3(Module): Bernoulli2.
                                RDK Ultra: Bayes.
                                RDK X5(Module): Bayes-e.
                                RDK S100: Nash-e.
                                RDK S100P: Nash-m.""") 
    parser.add_argument('--img-save-path', type=str, default='py_result.jpg', help='Path to Save Result Image.')
    parser.add_argument('--classes-num', type=int, default=1, help='Classes Num to Detect.')
    parser.add_argument('--nms-thres', type=float, default=0.7, help='IoU threshold.')
    parser.add_argument('--score-thres', type=float, default=0.25, help='confidence threshold.')
    parser.add_argument('--reg', type=int, default=16, help='DFL reg layer.')
    parser.add_argument('--nkpt', type=int, default=17, help='num of keypoints.')
    parser.add_argument('--kpt-conf-thres', type=float, default=0.5, help='keypoint confidence threshold.')
    parser.add_argument('--strides', type=lambda s: list(map(int, s.split(','))), 
                        default=[8, 16 ,32],
                        help='--strides 8, 16, 32')
    
    # å§¿æ€æ£€æµ‹ç›¸å…³å‚æ•°
    # Pose detection related parameters
    parser.add_argument('--knee-angle-thres', type=float, default=165, 
                        help='Knee angle threshold for crouching detection (degrees, < threshold = crouching).')
    parser.add_argument('--hip-angle-thres', type=float, default=160, 
                        help='Hip angle threshold for crouching detection (degrees, < threshold = crouching).')
    parser.add_argument('--min-kpt-conf', type=float, default=0.15, 
                        help='Minimum keypoint confidence for pose analysis.')
    parser.add_argument('--min-valid-kpts', type=int, default=2, 
                        help='Minimum number of valid keypoints required for pose analysis.')
    parser.add_argument('--display-kpt-conf', type=float, default=0.1, 
                        help='Minimum keypoint confidence for display (lower than analysis threshold).')
    parser.add_argument('--show-angles', action='store_true', default=True,
                        help='Show angle indicators and measurements.')
    
    # ç›¸æœºç›¸å…³å‚æ•°
    # Camera related parameters
    parser.add_argument('--camera-id', type=int, default=0, help='Camera device ID (default: 0).')
    
    opt = parser.parse_args()
    return opt

def get_camera_properties(camera_id):
    """
    ä½¿ç”¨OpenCVè·å–ç›¸æœºçš„å®é™…å‚æ•°
    Get actual camera parameters using OpenCV
    
    Args:
        camera_id: ç›¸æœºè®¾å¤‡ID / Camera device ID
        
    Returns:
        tuple: (actual_width, actual_height, actual_fps, success)
    """
    cap = cv2.VideoCapture(camera_id)
    
    if not cap.isOpened():
        logger.error(f"âŒ æ— æ³•æ‰“å¼€ç›¸æœºè®¾å¤‡ {camera_id}")
        return None, None, None, False
    
    try:
        # è·å–ç›¸æœºå®é™…å‚æ•°
        actual_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        actual_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        actual_fps = cap.get(cv2.CAP_PROP_FPS)
        
        logger.info(f"ğŸ“¹ ç›¸æœº {camera_id} å½“å‰å‚æ•°:")
        logger.info(f"   å®½åº¦: {actual_width}")
        logger.info(f"   é«˜åº¦: {actual_height}")
        logger.info(f"   å¸§ç‡: {actual_fps}")
        
        return actual_width, actual_height, actual_fps, True
        
    except Exception as e:
        logger.error(f"âŒ è·å–ç›¸æœºå‚æ•°æ—¶å‡ºé”™: {e}")
        return None, None, None, False
    finally:
        cap.release()

def calculate_angle(point1, point2, point3):
    """
    è®¡ç®—ä¸‰ç‚¹æ„æˆçš„è§’åº¦ (point2ä¸ºé¡¶ç‚¹)
    Calculate angle formed by three points (point2 is the vertex)
    
    Args:
        point1: (x1, y1) ç¬¬ä¸€ä¸ªç‚¹
        point2: (x2, y2) é¡¶ç‚¹
        point3: (x3, y3) ç¬¬ä¸‰ä¸ªç‚¹
    
    Returns:
        float: è§’åº¦ (åº¦æ•°)
    """
    # è®¡ç®—ä¸¤ä¸ªå‘é‡
    vector1 = (point1[0] - point2[0], point1[1] - point2[1])
    vector2 = (point3[0] - point2[0], point3[1] - point2[1])
    
    # è®¡ç®—å‘é‡é•¿åº¦
    len1 = sqrt(vector1[0]**2 + vector1[1]**2)
    len2 = sqrt(vector2[0]**2 + vector2[1]**2)
    
    if len1 == 0 or len2 == 0:
        return 0
    
    # è®¡ç®—ä½™å¼¦å€¼
    dot_product = vector1[0] * vector2[0] + vector1[1] * vector2[1]
    cos_angle = dot_product / (len1 * len2)
    
    # é˜²æ­¢æ•°å€¼è¯¯å·®
    cos_angle = max(-1, min(1, cos_angle))
    
    # è½¬æ¢ä¸ºè§’åº¦
    angle = math.acos(cos_angle) * 180 / math.pi
    return angle

def get_valid_keypoints(kpts, min_conf=0.15):
    """
    è·å–ç½®ä¿¡åº¦è¶³å¤Ÿé«˜çš„å…³é”®ç‚¹
    Get keypoints with sufficient confidence
    
    Args:
        kpts: å…³é”®ç‚¹åˆ—è¡¨ [(x, y, conf), ...]
        min_conf: æœ€å°ç½®ä¿¡åº¦
    
    Returns:
        dict: {keypoint_id: (x, y, conf)}
    """
    valid_kpts = {}
    for i, (x, y, conf) in enumerate(kpts):
        if conf >= min_conf:
            valid_kpts[i] = (x, y, conf)
    return valid_kpts

def analyze_pose_status(kpts, knee_angle_thres=165, hip_angle_thres=160, min_conf=0.15):
    """
    åŸºäºå…³é”®ç‚¹è§’åº¦åˆ†æäººä½“å§¿æ€ï¼šç«™ç«‹æˆ–è¹²ä¸‹
    Analyze human pose based on joint angles: standing or crouching
    
    Args:
        kpts: 17ä¸ªå…³é”®ç‚¹ [(x, y, conf), ...]
        knee_angle_thres: è†ç›–è§’åº¦é˜ˆå€¼ï¼Œå°äºæ­¤å€¼åˆ¤æ–­ä¸ºè¹²ä¸‹
        hip_angle_thres: é«‹éƒ¨è§’åº¦é˜ˆå€¼ï¼Œå°äºæ­¤å€¼åˆ¤æ–­ä¸ºè¹²ä¸‹
        min_conf: å…³é”®ç‚¹æœ€å°ç½®ä¿¡åº¦
    
    Returns:
        dict: åŒ…å«å§¿æ€åˆ†æç»“æœçš„å­—å…¸
    """
    # è·å–æœ‰æ•ˆå…³é”®ç‚¹
    valid_kpts = get_valid_keypoints(kpts, min_conf)
    
    # å®šä¹‰å…³é”®ç‚¹ç´¢å¼• (COCOæ ¼å¼)
    # 5: left_shoulder, 6: right_shoulder
    # 11: left_hip, 12: right_hip
    # 13: left_knee, 14: right_knee
    # 15: left_ankle, 16: right_ankle
    
    result = {
        'status': 'unknown',
        'confidence': 0.0,
        'knee_angles': {'left': None, 'right': None},
        'hip_angles': {'left': None, 'right': None},
        'crouch_indicators': [],
        'valid_keypoints_count': len(valid_kpts),
        'analysis_valid': False
    }
    
    # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„å…³é”®ç‚¹è¿›è¡Œåˆ†æ
    if len(valid_kpts) < 3:
        logger.debug(f"å…³é”®ç‚¹æ•°é‡ä¸è¶³è¿›è¡Œå§¿æ€åˆ†æ: {len(valid_kpts)}")
        return result
    
    crouch_indicators = []
    angle_confidences = []
    
    # è®¡ç®—å·¦è†ç›–è§’åº¦ (é«‹-è†-è¸)
    if 11 in valid_kpts and 13 in valid_kpts and 15 in valid_kpts:
        left_hip = (valid_kpts[11][0], valid_kpts[11][1])
        left_knee = (valid_kpts[13][0], valid_kpts[13][1])
        left_ankle = (valid_kpts[15][0], valid_kpts[15][1])
        
        left_knee_angle = calculate_angle(left_hip, left_knee, left_ankle)
        result['knee_angles']['left'] = left_knee_angle
        
        if left_knee_angle < knee_angle_thres:
            crouch_indicators.append(f"å·¦è†å¼¯æ›²({left_knee_angle:.1f}Â°)")
            # è§’åº¦è¶Šå°ï¼Œç½®ä¿¡åº¦è¶Šé«˜
            angle_conf = min(1.0, (knee_angle_thres - left_knee_angle) / knee_angle_thres + 0.3)
            angle_confidences.append(angle_conf)
        
        logger.debug(f"å·¦è†ç›–è§’åº¦: {left_knee_angle:.1f}Â°")
    
    # è®¡ç®—å³è†ç›–è§’åº¦ (é«‹-è†-è¸)
    if 12 in valid_kpts and 14 in valid_kpts and 16 in valid_kpts:
        right_hip = (valid_kpts[12][0], valid_kpts[12][1])
        right_knee = (valid_kpts[14][0], valid_kpts[14][1])
        right_ankle = (valid_kpts[16][0], valid_kpts[16][1])
        
        right_knee_angle = calculate_angle(right_hip, right_knee, right_ankle)
        result['knee_angles']['right'] = right_knee_angle
        
        if right_knee_angle < knee_angle_thres:
            crouch_indicators.append(f"å³è†å¼¯æ›²({right_knee_angle:.1f}Â°)")
            angle_conf = min(1.0, (knee_angle_thres - right_knee_angle) / knee_angle_thres + 0.3)
            angle_confidences.append(angle_conf)
        
        logger.debug(f"å³è†ç›–è§’åº¦: {right_knee_angle:.1f}Â°")
    
    # è®¡ç®—å·¦é«‹è§’åº¦ (è‚©-é«‹-è†)
    if 5 in valid_kpts and 11 in valid_kpts and 13 in valid_kpts:
        left_shoulder = (valid_kpts[5][0], valid_kpts[5][1])
        left_hip = (valid_kpts[11][0], valid_kpts[11][1])
        left_knee = (valid_kpts[13][0], valid_kpts[13][1])
        
        left_hip_angle = calculate_angle(left_shoulder, left_hip, left_knee)
        result['hip_angles']['left'] = left_hip_angle
        
        if left_hip_angle < hip_angle_thres:
            crouch_indicators.append(f"å·¦é«‹å¼¯æ›²({left_hip_angle:.1f}Â°)")
            angle_conf = min(1.0, (hip_angle_thres - left_hip_angle) / hip_angle_thres + 0.3)
            angle_confidences.append(angle_conf)
        
        logger.debug(f"å·¦é«‹è§’åº¦: {left_hip_angle:.1f}Â°")
    
    # è®¡ç®—å³é«‹è§’åº¦ (è‚©-é«‹-è†)
    if 6 in valid_kpts and 12 in valid_kpts and 14 in valid_kpts:
        right_shoulder = (valid_kpts[6][0], valid_kpts[6][1])
        right_hip = (valid_kpts[12][0], valid_kpts[12][1])
        right_knee = (valid_kpts[14][0], valid_kpts[14][1])
        
        right_hip_angle = calculate_angle(right_shoulder, right_hip, right_knee)
        result['hip_angles']['right'] = right_hip_angle
        
        if right_hip_angle < hip_angle_thres:
            crouch_indicators.append(f"å³é«‹å¼¯æ›²({right_hip_angle:.1f}Â°)")
            angle_conf = min(1.0, (hip_angle_thres - right_hip_angle) / hip_angle_thres + 0.3)
            angle_confidences.append(angle_conf)
        
        logger.debug(f"å³é«‹è§’åº¦: {right_hip_angle:.1f}Â°")
    
    # åˆ¤æ–­å§¿æ€
    result['crouch_indicators'] = crouch_indicators
    
    if len(crouch_indicators) > 0:
        # æœ‰å¼¯æ›²æŒ‡æ ‡ï¼Œåˆ¤æ–­ä¸ºè¹²ä¸‹
        status = 'crouching'
        # ç½®ä¿¡åº¦åŸºäºè§’åº¦åå·®ç¨‹åº¦å’ŒæŒ‡æ ‡æ•°é‡
        if angle_confidences:
            confidence = min(1.0, sum(angle_confidences) / len(angle_confidences) + len(crouch_indicators) * 0.1)
        else:
            confidence = 0.6
    else:
        # æ²¡æœ‰æ˜æ˜¾å¼¯æ›²ï¼Œåˆ¤æ–­ä¸ºç«™ç«‹
        status = 'standing'
        confidence = 0.8  # åŸºç¡€ç½®ä¿¡åº¦
    
    result.update({
        'status': status,
        'confidence': confidence,
        'analysis_valid': True
    })
    
    logger.debug(f"å§¿æ€åˆ†æ: {status} (ç½®ä¿¡åº¦: {confidence:.2f}, æŒ‡æ ‡: {crouch_indicators})")
    return result

def detect_pose_status(detection_results, opt):
    """
    ä»æ£€æµ‹ç»“æœä¸­åˆ†æäººä½“å§¿æ€çŠ¶æ€
    Analyze pose status from detection results
    
    Args:
        detection_results: YOLOæ£€æµ‹ç»“æœåˆ—è¡¨ï¼Œæ ¼å¼ä¸º (class_id, score, x1, y1, x2, y2, kpts)
        opt: å‘½ä»¤è¡Œå‚æ•°
    
    Returns:
        list: å§¿æ€åˆ†æç»“æœåˆ—è¡¨
    """
    pose_results = []
    person_class_id = 0  # COCOæ•°æ®é›†ä¸­personçš„class_idä¸º0
    
    for detection in detection_results:
        class_id, score, x1, y1, x2, y2, kpts = detection
        
        # åªå¤„ç†äººçš„æ£€æµ‹ç»“æœï¼Œä¸”ç½®ä¿¡åº¦è¦é«˜äºé˜ˆå€¼
        if class_id == person_class_id and score >= opt.score_thres:
            # åˆ†æå§¿æ€
            pose_analysis = analyze_pose_status(
                kpts, 
                opt.knee_angle_thres, 
                opt.hip_angle_thres,
                opt.min_kpt_conf
            )
            
            # åˆ›å»ºç»“æœï¼Œä¸ç®¡å§¿æ€åˆ†ææ˜¯å¦æœ‰æ•ˆéƒ½è¦æ˜¾ç¤º
            pose_result = {
                'bbox': (x1, y1, x2, y2),
                'detection_score': score,
                'keypoints': kpts,
                'pose_analysis': pose_analysis,
                'center': ((x1 + x2) // 2, (y1 + y2) // 2)
            }
            pose_results.append(pose_result)
            
            # åªæœ‰å§¿æ€åˆ†ææœ‰æ•ˆæ—¶æ‰è¾“å‡ºå§¿æ€ä¿¡æ¯
            if pose_analysis['analysis_valid']:
                indicators_str = ", ".join(pose_analysis['crouch_indicators']) if pose_analysis['crouch_indicators'] else "æ— å¼¯æ›²"
                logger.info(f"ğŸ§ æ£€æµ‹åˆ°äººä½“å§¿æ€: {pose_analysis['status']} "
                           f"(æ£€æµ‹ç½®ä¿¡åº¦={score:.3f}, å§¿æ€ç½®ä¿¡åº¦={pose_analysis['confidence']:.3f}, "
                           f"æŒ‡æ ‡={indicators_str})")
            else:
                logger.info(f"ğŸ‘¤ æ£€æµ‹åˆ°äººä½“ (æ£€æµ‹ç½®ä¿¡åº¦={score:.3f}, å…³é”®ç‚¹={pose_analysis['valid_keypoints_count']}, å§¿æ€åˆ†ææ— æ•ˆ)")
    
    return pose_results

def get_robot_command(pose_results, image_width, image_height, 
                      forward_zone_ratio=0.3, dead_zone_ratio=0.3, 
                      min_target_size_ratio=0.15):
    """
    æ ¹æ®å§¿æ€æ£€æµ‹ç»“æœç”Ÿæˆæœºå™¨äººæ§åˆ¶æŒ‡ä»¤
    Generate robot control commands based on pose detection results
    
    Args:
        pose_results: å§¿æ€æ£€æµ‹ç»“æœåˆ—è¡¨
        image_width: å›¾åƒå®½åº¦
        image_height: å›¾åƒé«˜åº¦
        forward_zone_ratio: å‰è¿›åŒºåŸŸæ¯”ä¾‹ï¼ˆä¸­å¿ƒåŒºåŸŸèŒƒå›´ï¼‰
        dead_zone_ratio: æ°´å¹³æ­»åŒºæ¯”ä¾‹ï¼ˆå·¦å³ç§»åŠ¨çš„æ­»åŒºï¼‰
        min_target_size_ratio: æœ€å°ç›®æ ‡å¤§å°æ¯”ä¾‹ï¼ˆåˆ¤æ–­æ˜¯å¦è¶³å¤Ÿè¿‘ï¼‰
    
    Returns:
        tuple: (command, target_person_info, command_reason)
        command: 'left', 'right', 'forward', 'rotate' å››ç§æŒ‡ä»¤ä¹‹ä¸€
        target_person_info: ç›®æ ‡äººç‰©ä¿¡æ¯ï¼ŒNoneè¡¨ç¤ºæ— ç›®æ ‡
        command_reason: æŒ‡ä»¤åŸå› è¯´æ˜
    """
    # ç­›é€‰è¹²ç€çš„äººï¼ˆåªé€‰æ‹©å§¿æ€åˆ†ææœ‰æ•ˆçš„ï¼‰
    crouching_persons = [
        person for person in pose_results 
        if person['pose_analysis']['analysis_valid'] and person['pose_analysis']['status'] == 'crouching'
    ]
    
    # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°è¹²ç€çš„äººï¼ŒåŸåœ°æ—‹è½¬æ‰¾äºº
    if not crouching_persons:
        # æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–æ£€æµ‹åˆ°çš„äººï¼ˆä½†ä¸æ˜¯è¹²ç€çš„ï¼‰
        all_persons = [person for person in pose_results if person['detection_score'] >= 0.3]
        if all_persons:
            reason = f"æ£€æµ‹åˆ°{len(all_persons)}ä¸ªäººï¼Œä½†éƒ½ä¸æ˜¯è¹²ç€çš„ï¼Œç»§ç»­æ—‹è½¬å¯»æ‰¾è¹²ç€çš„ç›®æ ‡"
        else:
            reason = "æœªæ£€æµ‹åˆ°ä»»ä½•äººï¼Œæ—‹è½¬å¯»æ‰¾ç›®æ ‡"
        
        logger.info(f"ğŸ”„ æœºå™¨äººæŒ‡ä»¤: åŸåœ°æ—‹è½¬ ({reason})")
        return 'rotate', None, reason
    
    # é€‰æ‹©æœ€ä½³ç›®æ ‡ï¼šä¼˜å…ˆé€‰æ‹©å§¿æ€ç½®ä¿¡åº¦æœ€é«˜çš„è¹²ç€çš„äºº
    if len(crouching_persons) > 1:
        target_person = max(crouching_persons, key=lambda x: x['pose_analysis']['confidence'])
        logger.info(f"ğŸ¯ æ£€æµ‹åˆ°{len(crouching_persons)}ä¸ªè¹²ç€çš„ç›®æ ‡ï¼Œé€‰æ‹©ç½®ä¿¡åº¦æœ€é«˜çš„")
    else:
        target_person = crouching_persons[0]
    
    # è·å–ç›®æ ‡ä¿¡æ¯
    x1, y1, x2, y2 = target_person['bbox']
    target_center_x = target_person['center'][0]
    target_center_y = target_person['center'][1]
    
    # è®¡ç®—ç›®æ ‡å¤§å°ï¼ˆç”¨äºåˆ¤æ–­è·ç¦»ï¼‰
    target_width = x2 - x1
    target_height = y2 - y1
    target_size_ratio = (target_width * target_height) / (image_width * image_height)
    
    # è®¡ç®—åŒºåŸŸè¾¹ç•Œ
    image_center_x = image_width // 2
    image_center_y = image_height // 2
    
    # æ°´å¹³æ­»åŒºï¼ˆå·¦å³ç§»åŠ¨çš„æ­»åŒºï¼‰
    horizontal_dead_zone = image_width * dead_zone_ratio / 2
    left_boundary = image_center_x - horizontal_dead_zone
    right_boundary = image_center_x + horizontal_dead_zone
    
    # å‰è¿›åŒºåŸŸï¼ˆä¸­å¿ƒåŒºåŸŸï¼‰
    forward_zone = image_width * forward_zone_ratio / 2
    forward_left = image_center_x - forward_zone
    forward_right = image_center_x + forward_zone
    
    # å†³ç­–é€»è¾‘
    pose_conf = target_person['pose_analysis']['confidence']
    detection_conf = target_person['detection_score']
    
    # 1. é¦–å…ˆåˆ¤æ–­ç›®æ ‡æ˜¯å¦åœ¨å‰è¿›åŒºåŸŸå†…
    if forward_left <= target_center_x <= forward_right:
        # ç›®æ ‡åœ¨ä¸­å¿ƒåŒºåŸŸï¼Œæ£€æŸ¥æ˜¯å¦è¶³å¤Ÿè¿‘
        if target_size_ratio >= min_target_size_ratio:
            # ç›®æ ‡è¶³å¤Ÿå¤§ï¼ˆè¿‘ï¼‰ï¼Œç»§ç»­å‰è¿›
            reason = f"ç›®æ ‡åœ¨ä¸­å¿ƒä¸”è¶³å¤Ÿè¿‘(å¤§å°æ¯”ä¾‹:{target_size_ratio:.3f}), ç»§ç»­å‰è¿›"
            command = 'forward'
        else:
            # ç›®æ ‡åœ¨ä¸­å¿ƒä½†è¿˜ä¸å¤Ÿè¿‘ï¼Œå‰è¿›æ¥è¿‘
            reason = f"ç›®æ ‡åœ¨ä¸­å¿ƒä½†è·ç¦»è¾ƒè¿œ(å¤§å°æ¯”ä¾‹:{target_size_ratio:.3f}), å‰è¿›æ¥è¿‘"
            command = 'forward'
    else:
        # 2. ç›®æ ‡ä¸åœ¨å‰è¿›åŒºåŸŸï¼Œéœ€è¦è°ƒæ•´æ–¹å‘
        if target_center_x < left_boundary:
            # ç›®æ ‡åœ¨å·¦ä¾§ï¼Œå‘å·¦ç§»åŠ¨
            reason = f"ç›®æ ‡åœ¨å·¦ä¾§(x={target_center_x}, å·¦è¾¹ç•Œ={left_boundary:.0f}), å‘å·¦è°ƒæ•´"
            command = 'left'
        elif target_center_x > right_boundary:
            # ç›®æ ‡åœ¨å³ä¾§ï¼Œå‘å³ç§»åŠ¨
            reason = f"ç›®æ ‡åœ¨å³ä¾§(x={target_center_x}, å³è¾¹ç•Œ={right_boundary:.0f}), å‘å³è°ƒæ•´"
            command = 'right'
        else:
            # ç›®æ ‡åœ¨æ­»åŒºå†…ï¼Œå‰è¿›
            reason = f"ç›®æ ‡åœ¨æ°´å¹³æ­»åŒºå†…(x={target_center_x}), å‰è¿›æ¥è¿‘"
            command = 'forward'
    
    logger.info(f"ğŸ¤– æœºå™¨äººæŒ‡ä»¤: {command.upper()} - {reason}")
    logger.debug(f"   ç›®æ ‡ä½ç½®: ({target_center_x}, {target_center_y})")
    logger.debug(f"   ç›®æ ‡å¤§å°: {target_width}x{target_height} (æ¯”ä¾‹:{target_size_ratio:.3f})")
    logger.debug(f"   å§¿æ€ç½®ä¿¡åº¦: {pose_conf:.3f}, æ£€æµ‹ç½®ä¿¡åº¦: {detection_conf:.3f}")
    
    return command, target_person, reason

def get_robot_direction_command(pose_results, image_width, target_status='crouching', dead_zone_ratio=0.3):
    """
    å‘åå…¼å®¹çš„å‡½æ•°ï¼Œè°ƒç”¨æ–°çš„get_robot_commandå‡½æ•°
    Backward compatible function that calls the new get_robot_command function
    
    Args:
        pose_results: å§¿æ€æ£€æµ‹ç»“æœåˆ—è¡¨
        image_width: å›¾åƒå®½åº¦
        target_status: ç›®æ ‡å§¿æ€ (ä¿æŒå…¼å®¹æ€§ï¼Œå®é™…åªå¤„ç†'crouching')
        dead_zone_ratio: ä¸­å¿ƒæ­»åŒºæ¯”ä¾‹
    
    Returns:
        tuple: (direction, target_person_info) - ä¿æŒåŸæœ‰æ¥å£å…¼å®¹
    """
    # ä½¿ç”¨é»˜è®¤å›¾åƒé«˜åº¦ï¼ˆå¦‚æœæ²¡æœ‰æä¾›ï¼‰
    image_height = int(image_width * 0.75)  # å‡è®¾4:3æ¯”ä¾‹
    
    command, target_person, reason = get_robot_command(
        pose_results, image_width, image_height, 
        dead_zone_ratio=dead_zone_ratio
    )
    
    # æ˜ å°„æ–°æŒ‡ä»¤åˆ°æ—§æŒ‡ä»¤æ ¼å¼
    if command == 'rotate':
        direction = 'stop'  # æ—§ç‰ˆæœ¬ç”¨stopè¡¨ç¤º
    elif command == 'forward':
        direction = 'stop'  # æ—§ç‰ˆæœ¬æ²¡æœ‰forwardï¼Œæ˜ å°„ä¸ºstop
    else:
        direction = command  # left, rightä¿æŒä¸å˜
    
    return direction, target_person

def draw_detection(img: np.array, 
                   bbox: tuple[int, int, int, int],
                   score: float, 
                   class_id: int) -> None:
    """
    ç»˜åˆ¶æ£€æµ‹æ¡†å’Œæ ‡ç­¾
    Draw detection bounding box and label
    
    Args:
        img: è¾“å…¥å›¾åƒ
        bbox: æ£€æµ‹æ¡†åæ ‡ (x1, y1, x2, y2)
        score: æ£€æµ‹ç½®ä¿¡åº¦
        class_id: ç±»åˆ«ID
    """
    x1, y1, x2, y2 = bbox
    color = rdk_colors[class_id % 20]
    cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)
    label = f"{coco_names[class_id]}: {score:.2f}"
    (label_width, label_height), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)
    label_x, label_y = x1, y1 - 10 if y1 - 10 > label_height else y1 + 10
    cv2.rectangle(
        img, (label_x, label_y - label_height), (label_x + label_width, label_y + label_height), color, cv2.FILLED
    )
    cv2.putText(img, label, (label_x, label_y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)

def draw_keypoints_and_skeleton(img, kpts, min_conf=0.15):
    """
    ç»˜åˆ¶å…³é”®ç‚¹å’Œéª¨æ¶
    Draw keypoints and skeleton
    
    Args:
        img: è¾“å…¥å›¾åƒ
        kpts: å…³é”®ç‚¹åˆ—è¡¨ [(x, y, conf), ...]
        min_conf: æœ€å°ç½®ä¿¡åº¦é˜ˆå€¼
    """
    valid_kpts = get_valid_keypoints(kpts, min_conf)
    logger.debug(f"ç»˜åˆ¶å…³é”®ç‚¹: æ€»å…±{len(kpts)}ä¸ªï¼Œæœ‰æ•ˆ{len(valid_kpts)}ä¸ª (é˜ˆå€¼:{min_conf})")
    
    # ç»˜åˆ¶éª¨æ¶è¿æ¥
    for start_idx, end_idx in SKELETON_CONNECTIONS:
        if start_idx in valid_kpts and end_idx in valid_kpts:
            start_point = (int(valid_kpts[start_idx][0]), int(valid_kpts[start_idx][1]))
            end_point = (int(valid_kpts[end_idx][0]), int(valid_kpts[end_idx][1]))
            cv2.line(img, start_point, end_point, (0, 255, 0), 2)
    
    # ç»˜åˆ¶å…³é”®ç‚¹
    for idx, (x, y, conf) in valid_kpts.items():
        x, y = int(x), int(y)
        # æ ¹æ®å…³é”®ç‚¹ç±»å‹ä½¿ç”¨ä¸åŒé¢œè‰²
        if idx in [1, 2]:  # å¤´éƒ¨ (çœ¼ç›)
            color = (255, 0, 0)  # è“è‰²
        elif idx in [11, 12]:  # è…°éƒ¨ (é«‹)
            color = (0, 255, 0)  # ç»¿è‰²
        elif idx in [15, 16]:  # è„šéƒ¨ (è¸)
            color = (0, 0, 255)  # çº¢è‰²
        else:
            color = (255, 255, 0)  # é»„è‰²
        
        cv2.circle(img, (x, y), 4, color, -1)
        cv2.circle(img, (x, y), 6, (255, 255, 255), 1)

def draw_angle_indicators(img, kpts, min_conf=0.15, knee_angle_thres=165, hip_angle_thres=160):
    """
    ç»˜åˆ¶è§’åº¦æŒ‡ç¤ºå™¨ï¼ˆè¾…åŠ©çº¿ï¼‰
    Draw angle indicators (helper lines)
    
    Args:
        img: è¾“å…¥å›¾åƒ
        kpts: å…³é”®ç‚¹åˆ—è¡¨ [(x, y, conf), ...]
        min_conf: æœ€å°ç½®ä¿¡åº¦é˜ˆå€¼
        knee_angle_thres: è†ç›–è§’åº¦é˜ˆå€¼
        hip_angle_thres: é«‹éƒ¨è§’åº¦é˜ˆå€¼
    """
    valid_kpts = get_valid_keypoints(kpts, min_conf)
    
    # ç»˜åˆ¶å·¦è†ç›–è§’åº¦è¾…åŠ©çº¿
    if 11 in valid_kpts and 13 in valid_kpts and 15 in valid_kpts:
        left_hip = (int(valid_kpts[11][0]), int(valid_kpts[11][1]))
        left_knee = (int(valid_kpts[13][0]), int(valid_kpts[13][1]))
        left_ankle = (int(valid_kpts[15][0]), int(valid_kpts[15][1]))
        
        angle = calculate_angle(left_hip, left_knee, left_ankle)
        color = (0, 0, 255) if angle < knee_angle_thres else (0, 255, 0)  # çº¢è‰²=å¼¯æ›²ï¼Œç»¿è‰²=ç›´ç«‹
        
        # ç»˜åˆ¶è§’åº¦çº¿
        cv2.line(img, left_hip, left_knee, color, 2)
        cv2.line(img, left_knee, left_ankle, color, 2)
        
        # åœ¨è†ç›–ä½ç½®æ ‡æ³¨è§’åº¦
        cv2.putText(img, f"{angle:.0f}Â°", 
                   (left_knee[0] + 10, left_knee[1] - 10), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 1)
    
    # ç»˜åˆ¶å³è†ç›–è§’åº¦è¾…åŠ©çº¿
    if 12 in valid_kpts and 14 in valid_kpts and 16 in valid_kpts:
        right_hip = (int(valid_kpts[12][0]), int(valid_kpts[12][1]))
        right_knee = (int(valid_kpts[14][0]), int(valid_kpts[14][1]))
        right_ankle = (int(valid_kpts[16][0]), int(valid_kpts[16][1]))
        
        angle = calculate_angle(right_hip, right_knee, right_ankle)
        color = (0, 0, 255) if angle < knee_angle_thres else (0, 255, 0)
        
        cv2.line(img, right_hip, right_knee, color, 2)
        cv2.line(img, right_knee, right_ankle, color, 2)
        cv2.putText(img, f"{angle:.0f}Â°", 
                   (right_knee[0] + 10, right_knee[1] - 10), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 1)
    
    # ç»˜åˆ¶å·¦é«‹è§’åº¦è¾…åŠ©çº¿
    if 5 in valid_kpts and 11 in valid_kpts and 13 in valid_kpts:
        left_shoulder = (int(valid_kpts[5][0]), int(valid_kpts[5][1]))
        left_hip = (int(valid_kpts[11][0]), int(valid_kpts[11][1]))
        left_knee = (int(valid_kpts[13][0]), int(valid_kpts[13][1]))
        
        angle = calculate_angle(left_shoulder, left_hip, left_knee)
        color = (255, 0, 255) if angle < hip_angle_thres else (0, 255, 255)  # ç´«è‰²=å¼¯æ›²ï¼Œé’è‰²=ç›´ç«‹
        
        cv2.line(img, left_shoulder, left_hip, color, 1, cv2.LINE_4)
        cv2.line(img, left_hip, left_knee, color, 1, cv2.LINE_4)
        cv2.putText(img, f"{angle:.0f}Â°", 
                   (left_hip[0] - 30, left_hip[1] - 10), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 1)
    
    # ç»˜åˆ¶å³é«‹è§’åº¦è¾…åŠ©çº¿
    if 6 in valid_kpts and 12 in valid_kpts and 14 in valid_kpts:
        right_shoulder = (int(valid_kpts[6][0]), int(valid_kpts[6][1]))
        right_hip = (int(valid_kpts[12][0]), int(valid_kpts[12][1]))
        right_knee = (int(valid_kpts[14][0]), int(valid_kpts[14][1]))
        
        angle = calculate_angle(right_shoulder, right_hip, right_knee)
        color = (255, 0, 255) if angle < hip_angle_thres else (0, 255, 255)
        
        cv2.line(img, right_shoulder, right_hip, color, 1, cv2.LINE_4)
        cv2.line(img, right_hip, right_knee, color, 1, cv2.LINE_4)
        cv2.putText(img, f"{angle:.0f}Â°", 
                   (right_hip[0] + 10, right_hip[1] - 10), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 1)

def draw_pose_analysis(img, pose_result):
    """
    ç»˜åˆ¶å§¿æ€åˆ†æç»“æœ
    Draw pose analysis results
    
    Args:
        img: è¾“å…¥å›¾åƒ
        pose_result: å§¿æ€åˆ†æç»“æœ
    """
    bbox = pose_result['bbox']
    pose_analysis = pose_result['pose_analysis']
    
    x1, y1, x2, y2 = bbox
    
    # æ ¹æ®å§¿æ€çŠ¶æ€é€‰æ‹©é¢œè‰²
    if pose_analysis['status'] == 'standing':
        status_color = (0, 255, 0)  # ç»¿è‰²
        status_text = "STANDING"
    elif pose_analysis['status'] == 'crouching':
        status_color = (0, 0, 255)  # çº¢è‰²
        status_text = "CROUCHING"
    else:
        status_color = (128, 128, 128)  # ç°è‰²
        status_text = "UNKNOWN"
    
    # ç»˜åˆ¶å§¿æ€çŠ¶æ€æ¡†
    cv2.rectangle(img, (x1, y1), (x2, y2), status_color, 3)
    
    # ç»˜åˆ¶å§¿æ€ä¿¡æ¯
    info_lines = [
        f"{status_text}",
        f"Conf: {pose_analysis['confidence']:.2f}",
        f"KPts: {pose_analysis['valid_keypoints_count']}"
    ]
    
    # æ·»åŠ è§’åº¦ä¿¡æ¯
    if pose_analysis['knee_angles']['left'] is not None:
        info_lines.append(f"L_Knee: {pose_analysis['knee_angles']['left']:.0f}Â°")
    if pose_analysis['knee_angles']['right'] is not None:
        info_lines.append(f"R_Knee: {pose_analysis['knee_angles']['right']:.0f}Â°")
    if pose_analysis['hip_angles']['left'] is not None:
        info_lines.append(f"L_Hip: {pose_analysis['hip_angles']['left']:.0f}Â°")
    if pose_analysis['hip_angles']['right'] is not None:
        info_lines.append(f"R_Hip: {pose_analysis['hip_angles']['right']:.0f}Â°")
    
    for i, line in enumerate(info_lines):
        y_offset = y1 - 50 + i * 15
        if y_offset < 15:
            y_offset = y2 + 15 + i * 15
        cv2.putText(img, line, (x1, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.5, status_color, 2)

def detect_once(frame, model, opt):
    """
    æ‰§è¡Œä¸€æ¬¡æ£€æµ‹å¹¶è¿”å›ç»“æœ
    Perform one detection and return results
    
    Args:
        frame: è¾“å…¥å›¾åƒå¸§
        model: YOLOæ¨¡å‹å®ä¾‹
        opt: å‘½ä»¤è¡Œå‚æ•°
    
    Returns:
        tuple: (command, display_frame, pose_results, target_person, command_reason)
    """
    # æ¨¡å‹æ¨ç†
    input_tensor = model.preprocess_yuv420sp(frame.copy())
    outputs = model.c2numpy(model.forward(input_tensor))
    results = model.postProcess(outputs)
    
    # å§¿æ€åˆ†æ
    pose_results = detect_pose_status(results, opt)
    
    # ç”Ÿæˆæœºå™¨äººæ§åˆ¶æŒ‡ä»¤ (ä½¿ç”¨æ–°çš„å‡½æ•°)
    command, target_person, command_reason = get_robot_command(
        pose_results, frame.shape[1], frame.shape[0],
        forward_zone_ratio=0.3, dead_zone_ratio=0.3, min_target_size_ratio=0.15
    )
    
    # åˆ›å»ºæ˜¾ç¤ºå¸§
    display_frame = frame.copy()
    
    # ç»˜åˆ¶æ‰€æœ‰æ£€æµ‹ç»“æœå’Œå…³é”®ç‚¹
    for detection in results:
        class_id, score, x1, y1, x2, y2, kpts = detection
        if class_id == 0:  # åªç»˜åˆ¶äºº
            draw_detection(display_frame, (x1, y1, x2, y2), score, class_id)
            # ç»˜åˆ¶æ‰€æœ‰æ£€æµ‹åˆ°çš„å…³é”®ç‚¹å’Œéª¨æ¶ï¼ˆä¸ç®¡å§¿æ€åˆ†ææ˜¯å¦æˆåŠŸï¼‰
            draw_keypoints_and_skeleton(display_frame, kpts, opt.display_kpt_conf)
            # ç»˜åˆ¶è§’åº¦æŒ‡ç¤ºå™¨
            if opt.show_angles:
                draw_angle_indicators(display_frame, kpts, opt.min_kpt_conf, opt.knee_angle_thres, opt.hip_angle_thres)
    
    # ç»˜åˆ¶å§¿æ€åˆ†æç»“æœ
    standing_count = 0
    crouching_count = 0
    unknown_count = 0
    
    for pose_result in pose_results:
        # ç»˜åˆ¶å§¿æ€åˆ†æï¼ˆè¦†ç›–åœ¨å…³é”®ç‚¹ä¸Šæ–¹ï¼‰
        draw_pose_analysis(display_frame, pose_result)
        
        # ç»Ÿè®¡å§¿æ€ï¼ˆåªç»Ÿè®¡åˆ†ææœ‰æ•ˆçš„ï¼‰
        if pose_result['pose_analysis']['analysis_valid']:
            if pose_result['pose_analysis']['status'] == 'standing':
                standing_count += 1
            elif pose_result['pose_analysis']['status'] == 'crouching':
                crouching_count += 1
        else:
            unknown_count += 1
    
    # è®¡ç®—æ— æ³•åˆ†æå§¿æ€çš„äººæ•°
    total_persons = sum(1 for detection in results if detection[0] == 0 and detection[1] >= opt.score_thres)
    analyzed_persons = len(pose_results)
    unknown_count = total_persons - analyzed_persons
    
    # ç‰¹åˆ«æ ‡è®°ç›®æ ‡äººç‰©
    if target_person:
        x1, y1, x2, y2 = target_person['bbox']
        center_x, center_y = target_person['center']
        
        # é»„è‰²é«˜äº®è¾¹æ¡†
        cv2.rectangle(display_frame, (x1-5, y1-5), (x2+5, y2+5), (0, 255, 255), 4)
        # é»„è‰²ä¸­å¿ƒç‚¹
        cv2.circle(display_frame, (center_x, center_y), 8, (0, 255, 255), -1)
        # ç›®æ ‡æ ‡ç­¾
        target_label = "TARGET"
        cv2.putText(display_frame, target_label, (x1, y1-70), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    info_y = 30
    line_height = 35
    
    # å§¿æ€ç»Ÿè®¡
    if unknown_count > 0:
        stats_text = f"Standing: {standing_count}, Crouching: {crouching_count}, Unknown: {unknown_count}"
    else:
        stats_text = f"Standing: {standing_count}, Crouching: {crouching_count}"
    cv2.putText(display_frame, stats_text, (10, info_y), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
    
    # æœºå™¨äººæŒ‡ä»¤ - æ ¹æ®æŒ‡ä»¤ç±»å‹ä½¿ç”¨ä¸åŒé¢œè‰²
    info_y += line_height
    if command == 'forward':
        direction_color = (0, 255, 0)  # ç»¿è‰² - å‰è¿›
    elif command == 'rotate':
        direction_color = (255, 0, 255)  # ç´«è‰² - æ—‹è½¬
    elif command in ['left', 'right']:
        direction_color = (0, 165, 255)  # æ©™è‰² - å·¦å³ç§»åŠ¨
    else:
        direction_color = (128, 128, 128)  # ç°è‰² - å…¶ä»–
    
    direction_text = f"Robot Command: {command.upper()}"
    cv2.putText(display_frame, direction_text, (10, info_y), cv2.FONT_HERSHEY_SIMPLEX, 0.8, direction_color, 2)
    
    # æ˜¾ç¤ºæŒ‡ä»¤åŸå› 
    info_y += line_height - 10
    reason_text = f"Reason: {command_reason[:50]}..." if len(command_reason) > 50 else f"Reason: {command_reason}"
    cv2.putText(display_frame, reason_text, (10, info_y), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (200, 200, 200), 1)
    
    # ç»˜åˆ¶åŒºåŸŸæŒ‡ç¤ºå™¨
    image_width = frame.shape[1]
    image_height = frame.shape[0]
    image_center_x = image_width // 2
    
    # ç»˜åˆ¶å‰è¿›åŒºåŸŸï¼ˆä¸­å¿ƒåŒºåŸŸï¼‰
    forward_zone = image_width * 0.3 / 2
    forward_left = int(image_center_x - forward_zone)
    forward_right = int(image_center_x + forward_zone)
    
    # ç»˜åˆ¶æ°´å¹³æ­»åŒº
    dead_zone_width = image_width * 0.3 / 2
    left_boundary = int(image_center_x - dead_zone_width)
    right_boundary = int(image_center_x + dead_zone_width)
    
    # ç»˜åˆ¶åŒºåŸŸè¾¹ç•Œçº¿
    cv2.line(display_frame, (forward_left, 0), (forward_left, image_height), (0, 255, 0), 2)  # å‰è¿›åŒºåŸŸå·¦è¾¹ç•Œï¼ˆç»¿è‰²ï¼‰
    cv2.line(display_frame, (forward_right, 0), (forward_right, image_height), (0, 255, 0), 2)  # å‰è¿›åŒºåŸŸå³è¾¹ç•Œï¼ˆç»¿è‰²ï¼‰
    cv2.line(display_frame, (left_boundary, 0), (left_boundary, image_height), (255, 255, 0), 1)  # æ­»åŒºå·¦è¾¹ç•Œï¼ˆé»„è‰²ï¼‰
    cv2.line(display_frame, (right_boundary, 0), (right_boundary, image_height), (255, 255, 0), 1)  # æ­»åŒºå³è¾¹ç•Œï¼ˆé»„è‰²ï¼‰
    cv2.line(display_frame, (image_center_x, 0), (image_center_x, image_height), (255, 255, 255), 1)  # ä¸­å¿ƒçº¿ï¼ˆç™½è‰²ï¼‰
    
    # æ·»åŠ åŒºåŸŸæ ‡ç­¾
    cv2.putText(display_frame, "FORWARD ZONE", (forward_left + 10, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
    cv2.putText(display_frame, "LEFT", (10, image_height//2), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 165, 255), 2)
    cv2.putText(display_frame, "RIGHT", (image_width - 80, image_height//2), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 165, 255), 2)
    
    # æ·»åŠ æ“ä½œæç¤ºå’Œå›¾ä¾‹
    help_text = "Press 'q' to quit, 's' to save frame, 'i' for camera info"
    cv2.putText(display_frame, help_text, (10, image_height-60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)
    
    # æ·»åŠ æŒ‡ä»¤å›¾ä¾‹
    legend_text = f"Commands: GREEN=Forward, PURPLE=Rotate, ORANGE=Left/Right"
    cv2.putText(display_frame, legend_text, (10, image_height-40), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
    
    # æ·»åŠ è§’åº¦é¢œè‰²å›¾ä¾‹
    angle_legend = f"Angles: Red(<{opt.knee_angle_thres:.0f}Â°)=Bent, Green=Straight, Purple(<{opt.hip_angle_thres:.0f}Â°)=Hip Bent"
    cv2.putText(display_frame, angle_legend, (10, image_height-20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)

    return command, display_frame, pose_results, target_person, command_reason

def robot_interface(frame, model, opt):
    """
    ç®€åŒ–çš„æœºå™¨äººæ¥å£å‡½æ•° - ä¸“é—¨ç”¨äºä¸æœºå™¨äººç¨‹åºå¯¹æ¥
    Simplified robot interface function - specifically for robot program integration
    
    Args:
        frame: è¾“å…¥å›¾åƒå¸§ (OpenCV format)
        model: YOLOæ¨¡å‹å®ä¾‹
        opt: å‘½ä»¤è¡Œå‚æ•°æˆ–é…ç½®å¯¹è±¡
    
    Returns:
        dict: æœºå™¨äººæ§åˆ¶ä¿¡æ¯
        {
            'command': str,           # 'left', 'right', 'forward', 'rotate'
            'confidence': float,      # æŒ‡ä»¤ç½®ä¿¡åº¦ (0-1)
            'target_detected': bool,  # æ˜¯å¦æ£€æµ‹åˆ°è¹²ç€çš„ç›®æ ‡
            'target_info': dict,      # ç›®æ ‡ä¿¡æ¯ (å¦‚æœæœ‰)
            'reason': str            # æŒ‡ä»¤åŸå› 
        }
    """
    try:
        # æ‰§è¡Œæ£€æµ‹
        command, _, pose_results, target_person, command_reason = detect_once(frame, model, opt)
        
        # è®¡ç®—æŒ‡ä»¤ç½®ä¿¡åº¦
        if target_person and target_person['pose_analysis']['analysis_valid']:
            # æœ‰æœ‰æ•ˆç›®æ ‡æ—¶ï¼Œç½®ä¿¡åº¦åŸºäºå§¿æ€åˆ†æç½®ä¿¡åº¦
            confidence = target_person['pose_analysis']['confidence']
            target_detected = True
            target_info = {
                'position': target_person['center'],
                'bbox': target_person['bbox'],
                'detection_confidence': target_person['detection_score'],
                'pose_confidence': target_person['pose_analysis']['confidence'],
                'pose_status': target_person['pose_analysis']['status']
            }
        else:
            # æ— ç›®æ ‡æ—¶ï¼Œæ ¹æ®æŒ‡ä»¤ç±»å‹è®¾å®šç½®ä¿¡åº¦
            if command == 'rotate':
                confidence = 0.8  # æ—‹è½¬æŒ‡ä»¤æ¯”è¾ƒç¡®å®š
            else:
                confidence = 0.5  # å…¶ä»–æƒ…å†µä¿å®ˆä¼°è®¡
            target_detected = False
            target_info = None
        
        return {
            'command': command,
            'confidence': confidence,
            'target_detected': target_detected,
            'target_info': target_info,
            'reason': command_reason
        }
        
    except Exception as e:
        logger.error(f"âŒ æœºå™¨äººæ¥å£æ‰§è¡Œå‡ºé”™: {e}")
        # è¿”å›å®‰å…¨çš„é»˜è®¤å€¼
        return {
            'command': 'rotate',
            'confidence': 0.0,
            'target_detected': False,
            'target_info': None,
            'reason': f'æ£€æµ‹å‡ºé”™: {str(e)}'
        }

def create_robot_controller(model_path, **kwargs):
    """
    åˆ›å»ºæœºå™¨äººæ§åˆ¶å™¨çš„ä¾¿æ·å‡½æ•°
    Convenience function to create a robot controller
    
    Args:
        model_path: YOLOæ¨¡å‹è·¯å¾„
        **kwargs: å…¶ä»–é…ç½®å‚æ•°
    
    Returns:
        tuple: (model, opt) - å¯ç”¨äºrobot_interfaceå‡½æ•°
    """
    from argparse import Namespace
    
    # é»˜è®¤é…ç½®
    default_config = {
        'classes_num': 1,
        'nms_thres': 0.7,
        'score_thres': 0.25,
        'reg': 16,
        'nkpt': 17,
        'kpt_conf_thres': 0.5,
        'strides': [8, 16, 32],
        'knee_angle_thres': 140,
        'hip_angle_thres': 140,
        'min_kpt_conf': 0.2,
        'min_valid_kpts': 3,
        'display_kpt_conf': 0.1,
        'show_angles': True
    }
    
    # åˆå¹¶ç”¨æˆ·é…ç½®
    config = {**default_config, **kwargs}
    config['model_path'] = model_path
    
    # åˆ›å»ºå‘½åç©ºé—´å¯¹è±¡
    opt = Namespace(**config)
    
    # å¯¼å…¥å¹¶åˆ›å»ºæ¨¡å‹
    from model.yolo.model import Ultralytics_YOLO_Pose_Bayese_YUV420SP
    model = Ultralytics_YOLO_Pose_Bayese_YUV420SP(
        model_path=opt.model_path,
        classes_num=opt.classes_num,
        nms_thres=opt.nms_thres,
        score_thres=opt.score_thres,
        reg=opt.reg,
        strides=opt.strides,
        nkpt=opt.nkpt
    )
    
    logger.info("ğŸ¤– æœºå™¨äººæ§åˆ¶å™¨åˆ›å»ºæˆåŠŸ")
    return model, opt

# ä½¿ç”¨ç¤ºä¾‹ä»£ç ï¼ˆæ³¨é‡Šæ‰ï¼Œä»…ä¾›å‚è€ƒï¼‰
"""
ä½¿ç”¨ç¤ºä¾‹ / Usage Example:

# 1. åˆ›å»ºæœºå™¨äººæ§åˆ¶å™¨
model, opt = create_robot_controller('path/to/your/model.bin')

# 2. åœ¨ä¸»å¾ªç¯ä¸­ä½¿ç”¨
import cv2

cap = cv2.VideoCapture(0)
while True:
    ret, frame = cap.read()
    if not ret:
        break
    
    # è·å–æœºå™¨äººæ§åˆ¶æŒ‡ä»¤
    robot_cmd = robot_interface(frame, model, opt)
    
    # æ ¹æ®æŒ‡ä»¤æ§åˆ¶æœºå™¨äºº
    command = robot_cmd['command']
    confidence = robot_cmd['confidence']
    
    if confidence > 0.5:  # åªæœ‰åœ¨ç½®ä¿¡åº¦è¶³å¤Ÿé«˜æ—¶æ‰æ‰§è¡Œ
        if command == 'left':
            # æ§åˆ¶æœºå™¨äººå‘å·¦
            print("æœºå™¨äººå‘å·¦è½¬")
        elif command == 'right':
            # æ§åˆ¶æœºå™¨äººå‘å³
            print("æœºå™¨äººå‘å³è½¬")
        elif command == 'forward':
            # æ§åˆ¶æœºå™¨äººå‘å‰
            print("æœºå™¨äººå‘å‰ç§»åŠ¨")
        elif command == 'rotate':
            # æ§åˆ¶æœºå™¨äººåŸåœ°æ—‹è½¬
            print("æœºå™¨äººåŸåœ°æ—‹è½¬")
    
    print(f"æŒ‡ä»¤: {command}, ç½®ä¿¡åº¦: {confidence:.2f}, åŸå› : {robot_cmd['reason']}")

cap.release()
"""