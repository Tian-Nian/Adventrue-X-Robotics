import cv2
import argparse
import logging 
import numpy as np

logging.basicConfig(
    level = logging.DEBUG,
    format = '[%(name)s] [%(asctime)s.%(msecs)03d] [%(levelname)s] %(message)s',
    datefmt='%H:%M:%S')
logger = logging.getLogger("RDK_YOLO")

coco_names = [
    "person", "bicycle", "car", "motorcycle", "airplane", "bus", "train", "truck", "boat", "traffic light", 
    "fire hydrant", "stop sign", "parking meter", "bench", "bird", "cat", "dog", "horse", "sheep", "cow", 
    "elephant", "bear", "zebra", "giraffe", "backpack", "umbrella", "handbag", "tie", "suitcase", "frisbee", 
    "skis", "snowboard", "sports ball", "kite", "baseball bat", "baseball glove", "skateboard", "surfboard", "tennis racket", "bottle", 
    "wine glass", "cup", "fork", "knife", "spoon", "bowl", "banana", "apple", "sandwich", "orange", 
    "broccoli", "carrot", "hot dog", "pizza", "donut", "cake", "chair", "couch", "potted plant", "bed", 
    "dining table", "toilet", "tv", "laptop", "mouse", "remote", "keyboard", "cell phone", "microwave", "oven", 
    "toaster", "sink", "refrigerator", "book", "clock", "vase", "scissors", "teddy bear", "hair drier", "toothbrush"
    ]
    
rdk_colors = [
    (56, 56, 255), (151, 157, 255), (31, 112, 255), (29, 178, 255),(49, 210, 207), (10, 249, 72), (23, 204, 146), (134, 219, 61),
    (52, 147, 26), (187, 212, 0), (168, 153, 44), (255, 194, 0),(147, 69, 52), (255, 115, 100), (236, 24, 0), (255, 56, 132),
    (133, 0, 82), (255, 56, 203), (200, 149, 255), (199, 55, 255)]

def get_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--model-path', type=str, default='source/yolo11n_pose_bayese_640x640_nv12.bin', 
                        help="""Path to BPU Quantized *.bin Model.
                                RDK X3(Module): Bernoulli2.
                                RDK Ultra: Bayes.
                                RDK X5(Module): Bayes-e.
                                RDK S100: Nash-e.
                                RDK S100P: Nash-m.""") 
    parser.add_argument('--test-img', type=str, default='source/bus.jpg', help='Path to Load Test Image.')
    parser.add_argument('--img-save-path', type=str, default='py_result.jpg', help='Path to Load Test Image.')
    parser.add_argument('--classes-num', type=int, default=1, help='Classes Num to Detect.')
    parser.add_argument('--nms-thres', type=float, default=0.7, help='IoU threshold.')
    parser.add_argument('--score-thres', type=float, default=0.80, help='confidence threshold.')
    parser.add_argument('--reg', type=int, default=16, help='DFL reg layer.')
    parser.add_argument('--nkpt', type=int, default=17, help='num of keypoints.')
    parser.add_argument('--kpt-conf-thres', type=float, default=0.5, help='confidence threshold.')
    parser.add_argument('--strides', type=lambda s: list(map(int, s.split(','))), 
                        default=[8, 16 ,32],
                        help='--strides 8, 16, 32')
    
    parser.add_argument('--crouch-ratio', type=float, default=1.2, help='Aspect ratio threshold for crouching detection.')
    parser.add_argument('--min-crouch-conf', type=float, default=1.5, help='Minimum confidence for crouching person detection.')
    
    # ç›¸æœºç›¸å…³å‚æ•°
    parser.add_argument('--camera-mode', action='store_true', help='Enable real-time camera detection mode.')
    parser.add_argument('--camera-id', type=int, default=0, help='Camera device ID (default: 0).')
    parser.add_argument('--simple-display', action='store_true', help='Use simplified display with cv2_imshow support.')
    
    opt = parser.parse_args()

    return opt

def get_camera_properties(camera_id):
    """
    ä½¿ç”¨OpenCVè·å–ç›¸æœºçš„å®é™…å‚æ•°
    
    Args:
        camera_id: ç›¸æœºè®¾å¤‡ID
        
    Returns:
        tuple: (actual_width, actual_height, actual_fps, success)
    """
    cap = cv2.VideoCapture(camera_id)
    
    if not cap.isOpened():
        logger.error(f"âŒ æ— æ³•æ‰“å¼€ç›¸æœºè®¾å¤‡ {camera_id}")
        return None, None, None, False
    
    try:
        # è·å–ç›¸æœºå®é™…å‚æ•°
        actual_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        actual_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        actual_fps = cap.get(cv2.CAP_PROP_FPS)
        
        logger.info(f"ğŸ“¹ ç›¸æœº {camera_id} å½“å‰å‚æ•°:")
        logger.info(f"   å®½åº¦: {actual_width}")
        logger.info(f"   é«˜åº¦: {actual_height}")
        logger.info(f"   å¸§ç‡: {actual_fps}")
        
        # è·å–ç›¸æœºæ”¯æŒçš„å…¶ä»–å±æ€§
        brightness = cap.get(cv2.CAP_PROP_BRIGHTNESS)
        contrast = cap.get(cv2.CAP_PROP_CONTRAST)
        saturation = cap.get(cv2.CAP_PROP_SATURATION)
        
        logger.info(f"ğŸ“¹ ç›¸æœº {camera_id} å…¶ä»–å±æ€§:")
        logger.info(f"   äº®åº¦: {brightness}")
        logger.info(f"   å¯¹æ¯”åº¦: {contrast}")
        logger.info(f"   é¥±å’Œåº¦: {saturation}")
        
        # å°è¯•è·å–æ”¯æŒçš„åˆ†è¾¨ç‡èŒƒå›´
        logger.info(f"ğŸ“¹ æ­£åœ¨æµ‹è¯•ç›¸æœº {camera_id} æ”¯æŒçš„åˆ†è¾¨ç‡...")
        common_resolutions = [
            (320, 240),   # QVGA
            (640, 480),   # VGA
            (800, 600),   # SVGA
            (1024, 768),  # XGA
            (1280, 720),  # HD
            (1280, 960),  # SXGA
            (1920, 1080), # Full HD
        ]
        
        supported_resolutions = []
        for width, height in common_resolutions:
            cap.set(cv2.CAP_PROP_FRAME_WIDTH, width)
            cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)
            test_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            test_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            
            if test_width == width and test_height == height:
                supported_resolutions.append((width, height))
        
        if supported_resolutions:
            logger.info(f"ğŸ“¹ ç›¸æœº {camera_id} æ”¯æŒçš„åˆ†è¾¨ç‡:")
            for res in supported_resolutions:
                logger.info(f"   {res[0]}x{res[1]}")
        
        return actual_width, actual_height, actual_fps, True
        
    except Exception as e:
        logger.error(f"âŒ è·å–ç›¸æœºå‚æ•°æ—¶å‡ºé”™: {e}")
        return None, None, None, False
    finally:
        cap.release()

def detect_crouching_persons(detection_results, aspect_ratio_threshold=1.2, min_confidence=0.5):
    """
    ä»æ£€æµ‹ç»“æœä¸­è¯†åˆ«ç–‘ä¼¼è¹²ç€çš„äºº
    
    Args:
        detection_results: YOLOæ£€æµ‹ç»“æœåˆ—è¡¨ï¼Œæ ¼å¼ä¸º (class_id, score, x1, y1, x2, y2)
        aspect_ratio_threshold: é•¿å®½æ¯”é˜ˆå€¼ï¼Œå°äºæ­¤å€¼è®¤ä¸ºæ˜¯è¹²ç€çš„äºº
        min_confidence: æœ€å°ç½®ä¿¡åº¦é˜ˆå€¼
    
    Returns:
        crouching_persons: è¹²ç€çš„äººçš„ä¿¡æ¯åˆ—è¡¨ï¼Œæ ¼å¼ä¸º (score, x1, y1, x2, y2, center_x, center_y, width, height, aspect_ratio)
    """
    crouching_persons = []
    person_class_id = 0  # COCOæ•°æ®é›†ä¸­personçš„class_idä¸º0
    
    for class_id, score, x1, y1, x2, y2 in detection_results:
        # åªå¤„ç†äººçš„æ£€æµ‹ç»“æœï¼Œä¸”ç½®ä¿¡åº¦è¦é«˜äºé˜ˆå€¼
        if class_id == person_class_id and score >= min_confidence:
            # è®¡ç®—æ£€æµ‹æ¡†çš„å®½åº¦å’Œé«˜åº¦
            width = x2 - x1
            height = y2 - y1
            
            # é¿å…é™¤é›¶é”™è¯¯
            if width <= 0:
                continue
                
            # è®¡ç®—é•¿å®½æ¯”ï¼ˆé«˜åº¦/å®½åº¦ï¼‰
            aspect_ratio = height / width
            
            # å¦‚æœé•¿å®½æ¯”å°äºé˜ˆå€¼ï¼Œè®¤ä¸ºæ˜¯è¹²ç€çš„äºº
            if aspect_ratio < aspect_ratio_threshold:
                # è®¡ç®—ä¸­å¿ƒç‚¹åæ ‡
                center_x = (x1 + x2) // 2
                center_y = (y1 + y2) // 2
                
                crouching_person_info = {
                    'score': score,
                    'bbox': (x1, y1, x2, y2),
                    'center': (center_x, center_y),
                    'width': width,
                    'height': height,
                    'aspect_ratio': aspect_ratio
                }
                
                crouching_persons.append(crouching_person_info)
                
                logger.info(f"ğŸ” å‘ç°è¹²ç€çš„äºº: ç½®ä¿¡åº¦={score:.3f}, ä¸­å¿ƒç‚¹=({center_x}, {center_y}), "
                           f"å°ºå¯¸={width}x{height}, é•¿å®½æ¯”={aspect_ratio:.2f}")
    
    return crouching_persons

def get_robot_direction_command(crouching_persons, image_width, dead_zone_ratio=0.3):
    """
    æ ¹æ®è¹²ç€çš„äººçš„ä½ç½®ç”Ÿæˆæœºå™¨äººç§»åŠ¨æŒ‡ä»¤
    
    Args:
        crouching_persons: è¹²ç€çš„äººçš„ä¿¡æ¯åˆ—è¡¨
        image_width: å›¾åƒå®½åº¦
        dead_zone_ratio: ä¸­å¿ƒæ­»åŒºæ¯”ä¾‹ï¼ˆ0-1ï¼‰ï¼Œåœ¨æ­¤èŒƒå›´å†…ä¸ç§»åŠ¨
    
    Returns:
        direction: 'left', 'right', 'stop', 'multiple_targets'
    """
    if not crouching_persons:
        return 'stop'  # æ²¡æœ‰æ£€æµ‹åˆ°è¹²ç€çš„äºº
    
    if len(crouching_persons) > 1:
        # å¤šä¸ªç›®æ ‡æ—¶ï¼Œé€‰æ‹©ç½®ä¿¡åº¦æœ€é«˜çš„
        crouching_persons = [max(crouching_persons, key=lambda x: x['score'])]
        logger.info("ğŸ¯ æ£€æµ‹åˆ°å¤šä¸ªç›®æ ‡ï¼Œé€‰æ‹©ç½®ä¿¡åº¦æœ€é«˜çš„")
    
    # è·å–ç›®æ ‡çš„ä¸­å¿ƒxåæ ‡
    target = crouching_persons[0]
    target_center_x = target['center'][0]
    
    # è®¡ç®—å›¾åƒä¸­å¿ƒå’Œæ­»åŒºèŒƒå›´
    image_center = image_width // 2
    dead_zone_width = image_width * dead_zone_ratio / 2
    
    left_boundary = image_center - dead_zone_width
    right_boundary = image_center + dead_zone_width
    
    if target_center_x < left_boundary:
        direction = 'left'
        logger.info(f"ğŸ¤– æœºå™¨äººæŒ‡ä»¤: å‘å·¦ç§»åŠ¨ (ç›®æ ‡åœ¨ x={target_center_x}, å·¦è¾¹ç•Œ={left_boundary:.0f})")
    elif target_center_x > right_boundary:
        direction = 'right'
        logger.info(f"ğŸ¤– æœºå™¨äººæŒ‡ä»¤: å‘å³ç§»åŠ¨ (ç›®æ ‡åœ¨ x={target_center_x}, å³è¾¹ç•Œ={right_boundary:.0f})")
    else:
        direction = 'stop'
        logger.info(f"ğŸ¤– æœºå™¨äººæŒ‡ä»¤: åœæ­¢ç§»åŠ¨ (ç›®æ ‡åœ¨ä¸­å¿ƒåŒºåŸŸ x={target_center_x})")
    
    return direction

def draw_detection(img: np.array, 
                   bbox: tuple[int, int, int, int],
                   score: float, 
                   class_id: int) -> None:
    """
    Draws a detection bounding box and label on the image.

    Parameters:
        img (np.array): The input image.
        bbox (tuple[int, int, int, int]): A tuple containing the bounding box coordinates (x1, y1, x2, y2).
        score (float): The detection score of the object.
        class_id (int): The class ID of the detected object.
    """
    x1, y1, x2, y2 = bbox
    color = rdk_colors[class_id%20]
    cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)
    label = f"{coco_names[class_id]}: {score:.2f}"
    (label_width, label_height), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)
    label_x, label_y = x1, y1 - 10 if y1 - 10 > label_height else y1 + 10
    cv2.rectangle(
        img, (label_x, label_y - label_height), (label_x + label_width, label_y + label_height), color, cv2.FILLED
    )
    cv2.putText(img, label, (label_x, label_y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)

def detect_once(frame, model, opt):
    """
    å®æ—¶ç›¸æœºæ£€æµ‹è¹²ç€çš„äºº
    
    Args:
        model: YOLOv8æ£€æµ‹æ¨¡å‹å®ä¾‹
        opt: å‘½ä»¤è¡Œå‚æ•°
    """
    logger.info("æŒ‰ 'q' é”®é€€å‡ºï¼ŒæŒ‰ 's' é”®ä¿å­˜å½“å‰å¸§ï¼ŒæŒ‰ 'i' é”®æŸ¥çœ‹ç›¸æœºä¿¡æ¯")
    
    # æ£€æµ‹å¤„ç†
    input_tensor = model.preprocess_yuv420sp(frame.copy())
    outputs = model.c2numpy(model.forward(input_tensor))
    results = model.postProcess(outputs)
    
    # è¹²ç€çš„äººæ£€æµ‹
    crouching_persons = detect_crouching_persons(results, opt.crouch_ratio, opt.min_crouch_conf)
    
    # ç”Ÿæˆæœºå™¨äººç§»åŠ¨æŒ‡ä»¤
    direction = get_robot_direction_command(crouching_persons, frame.shape[1])
    
    # åœ¨å¸§ä¸Šç»˜åˆ¶æ£€æµ‹ç»“æœ
    display_frame = frame.copy()
    
    # ç»˜åˆ¶æ‰€æœ‰æ£€æµ‹ç»“æœï¼ˆåŠé€æ˜ï¼‰
    for class_id, score, x1, y1, x2, y2 in results:
        draw_detection(display_frame, (x1, y1, x2, y2), score, class_id)
    
    # ç‰¹åˆ«æ ‡è®°è¹²ç€çš„äºº
    if crouching_persons:
        for person_info in crouching_persons:
            x1, y1, x2, y2 = person_info['bbox']
            center_x, center_y = person_info['center']
            
            # çº¢è‰²è¾¹æ¡†æ ‡è®°è¹²ç€çš„äºº
            cv2.rectangle(display_frame, (x1, y1), (x2, y2), (0, 0, 255), 4)
            # é»„è‰²ä¸­å¿ƒç‚¹
            cv2.circle(display_frame, (center_x, center_y), 8, (0, 255, 255), -1)
            # è¹²ç€çš„äººæ ‡ç­¾
            crouch_label = f"CROUCHING: {person_info['score']:.2f} AR:{person_info['aspect_ratio']:.2f}"
            cv2.putText(display_frame, crouch_label, (x1, y1-35), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
    
    # æ˜¾ç¤ºä¿¡æ¯
    info_y = 30
    line_height = 35
    
    # æ£€æµ‹ç»“æœä¿¡æ¯
    info_y += line_height
    if crouching_persons:
        count_text = f"Crouching Persons: {len(crouching_persons)}"
        cv2.putText(display_frame, count_text, (10, info_y), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
    else:
        cv2.putText(display_frame, "No Crouching Person", (10, info_y), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)
    
    # æœºå™¨äººæŒ‡ä»¤ä¿¡æ¯
    info_y += line_height
    direction_color = (0, 255, 0) if direction != 'stop' else (0, 255, 255)
    
    direction_text = f"Robot Command: {direction.upper()}"
    cv2.putText(display_frame, direction_text, (10, info_y), cv2.FONT_HERSHEY_SIMPLEX, 0.8, direction_color, 2)
    
    # ç»˜åˆ¶ä¸­å¿ƒæ­»åŒº
    image_center = frame.shape[1] // 2
    dead_zone_width = frame.shape[1] * 0.3 / 2
    left_boundary = int(image_center - dead_zone_width)
    right_boundary = int(image_center + dead_zone_width)
    
    # ç»˜åˆ¶æ­»åŒºè¾¹ç•Œçº¿
    cv2.line(display_frame, (left_boundary, 0), (left_boundary, frame.shape[0]), (255, 255, 0), 2)
    cv2.line(display_frame, (right_boundary, 0), (right_boundary, frame.shape[0]), (255, 255, 0), 2)
    cv2.line(display_frame, (image_center, 0), (image_center, frame.shape[0]), (255, 255, 255), 1)
    
    # æ·»åŠ æ“ä½œæç¤º
    help_text = "Press 'q' to quit, 's' to save frame, 'i' for camera info"
    cv2.putText(display_frame, help_text, (10, frame.shape[0]-20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)

    return direction, display_frame