import cv2
import argparse
import logging 
import numpy as np

logging.basicConfig(
    level = logging.DEBUG,
    format = '[%(name)s] [%(asctime)s.%(msecs)03d] [%(levelname)s] %(message)s',
    datefmt='%H:%M:%S')
logger = logging.getLogger("RDK_YOLO")

coco_names = [
    "person", "bicycle", "car", "motorcycle", "airplane", "bus", "train", "truck", "boat", "traffic light", 
    "fire hydrant", "stop sign", "parking meter", "bench", "bird", "cat", "dog", "horse", "sheep", "cow", 
    "elephant", "bear", "zebra", "giraffe", "backpack", "umbrella", "handbag", "tie", "suitcase", "frisbee", 
    "skis", "snowboard", "sports ball", "kite", "baseball bat", "baseball glove", "skateboard", "surfboard", "tennis racket", "bottle", 
    "wine glass", "cup", "fork", "knife", "spoon", "bowl", "banana", "apple", "sandwich", "orange", 
    "broccoli", "carrot", "hot dog", "pizza", "donut", "cake", "chair", "couch", "potted plant", "bed", 
    "dining table", "toilet", "tv", "laptop", "mouse", "remote", "keyboard", "cell phone", "microwave", "oven", 
    "toaster", "sink", "refrigerator", "book", "clock", "vase", "scissors", "teddy bear", "hair drier", "toothbrush"
    ]
    
rdk_colors = [
    (56, 56, 255), (151, 157, 255), (31, 112, 255), (29, 178, 255),(49, 210, 207), (10, 249, 72), (23, 204, 146), (134, 219, 61),
    (52, 147, 26), (187, 212, 0), (168, 153, 44), (255, 194, 0),(147, 69, 52), (255, 115, 100), (236, 24, 0), (255, 56, 132),
    (133, 0, 82), (255, 56, 203), (200, 149, 255), (199, 55, 255)]

def get_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--model-path', type=str, default='source/yolo11n_pose_bayese_640x640_nv12.bin', 
                        help="""Path to BPU Quantized *.bin Model.
                                RDK X3(Module): Bernoulli2.
                                RDK Ultra: Bayes.
                                RDK X5(Module): Bayes-e.
                                RDK S100: Nash-e.
                                RDK S100P: Nash-m.""") 
    parser.add_argument('--test-img', type=str, default='source/bus.jpg', help='Path to Load Test Image.')
    parser.add_argument('--img-save-path', type=str, default='py_result.jpg', help='Path to Load Test Image.')
    parser.add_argument('--classes-num', type=int, default=1, help='Classes Num to Detect.')
    parser.add_argument('--nms-thres', type=float, default=0.7, help='IoU threshold.')
    parser.add_argument('--score-thres', type=float, default=0.80, help='confidence threshold.')
    parser.add_argument('--reg', type=int, default=16, help='DFL reg layer.')
    parser.add_argument('--nkpt', type=int, default=17, help='num of keypoints.')
    parser.add_argument('--kpt-conf-thres', type=float, default=0.5, help='confidence threshold.')
    parser.add_argument('--strides', type=lambda s: list(map(int, s.split(','))), 
                        default=[8, 16 ,32],
                        help='--strides 8, 16, 32')
    
    parser.add_argument('--crouch-ratio', type=float, default=1.2, help='Aspect ratio threshold for crouching detection.')
    parser.add_argument('--min-crouch-conf', type=float, default=1.5, help='Minimum confidence for crouching person detection.')
    
    # 相机相关参数
    parser.add_argument('--camera-mode', action='store_true', help='Enable real-time camera detection mode.')
    parser.add_argument('--camera-id', type=int, default=0, help='Camera device ID (default: 0).')
    parser.add_argument('--simple-display', action='store_true', help='Use simplified display with cv2_imshow support.')
    
    opt = parser.parse_args()

    return opt

def get_camera_properties(camera_id):
    """
    使用OpenCV获取相机的实际参数
    
    Args:
        camera_id: 相机设备ID
        
    Returns:
        tuple: (actual_width, actual_height, actual_fps, success)
    """
    cap = cv2.VideoCapture(camera_id)
    
    if not cap.isOpened():
        logger.error(f"❌ 无法打开相机设备 {camera_id}")
        return None, None, None, False
    
    try:
        # 获取相机实际参数
        actual_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        actual_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        actual_fps = cap.get(cv2.CAP_PROP_FPS)
        
        logger.info(f"📹 相机 {camera_id} 当前参数:")
        logger.info(f"   宽度: {actual_width}")
        logger.info(f"   高度: {actual_height}")
        logger.info(f"   帧率: {actual_fps}")
        
        # 获取相机支持的其他属性
        brightness = cap.get(cv2.CAP_PROP_BRIGHTNESS)
        contrast = cap.get(cv2.CAP_PROP_CONTRAST)
        saturation = cap.get(cv2.CAP_PROP_SATURATION)
        
        logger.info(f"📹 相机 {camera_id} 其他属性:")
        logger.info(f"   亮度: {brightness}")
        logger.info(f"   对比度: {contrast}")
        logger.info(f"   饱和度: {saturation}")
        
        # 尝试获取支持的分辨率范围
        logger.info(f"📹 正在测试相机 {camera_id} 支持的分辨率...")
        common_resolutions = [
            (320, 240),   # QVGA
            (640, 480),   # VGA
            (800, 600),   # SVGA
            (1024, 768),  # XGA
            (1280, 720),  # HD
            (1280, 960),  # SXGA
            (1920, 1080), # Full HD
        ]
        
        supported_resolutions = []
        for width, height in common_resolutions:
            cap.set(cv2.CAP_PROP_FRAME_WIDTH, width)
            cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)
            test_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            test_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            
            if test_width == width and test_height == height:
                supported_resolutions.append((width, height))
        
        if supported_resolutions:
            logger.info(f"📹 相机 {camera_id} 支持的分辨率:")
            for res in supported_resolutions:
                logger.info(f"   {res[0]}x{res[1]}")
        
        return actual_width, actual_height, actual_fps, True
        
    except Exception as e:
        logger.error(f"❌ 获取相机参数时出错: {e}")
        return None, None, None, False
    finally:
        cap.release()

def detect_crouching_persons(detection_results, aspect_ratio_threshold=1.2, min_confidence=0.5):
    """
    从检测结果中识别疑似蹲着的人
    
    Args:
        detection_results: YOLO检测结果列表，格式为 (class_id, score, x1, y1, x2, y2)
        aspect_ratio_threshold: 长宽比阈值，小于此值认为是蹲着的人
        min_confidence: 最小置信度阈值
    
    Returns:
        crouching_persons: 蹲着的人的信息列表，格式为 (score, x1, y1, x2, y2, center_x, center_y, width, height, aspect_ratio)
    """
    crouching_persons = []
    person_class_id = 0  # COCO数据集中person的class_id为0
    
    for class_id, score, x1, y1, x2, y2 in detection_results:
        # 只处理人的检测结果，且置信度要高于阈值
        if class_id == person_class_id and score >= min_confidence:
            # 计算检测框的宽度和高度
            width = x2 - x1
            height = y2 - y1
            
            # 避免除零错误
            if width <= 0:
                continue
                
            # 计算长宽比（高度/宽度）
            aspect_ratio = height / width
            
            # 如果长宽比小于阈值，认为是蹲着的人
            if aspect_ratio < aspect_ratio_threshold:
                # 计算中心点坐标
                center_x = (x1 + x2) // 2
                center_y = (y1 + y2) // 2
                
                crouching_person_info = {
                    'score': score,
                    'bbox': (x1, y1, x2, y2),
                    'center': (center_x, center_y),
                    'width': width,
                    'height': height,
                    'aspect_ratio': aspect_ratio
                }
                
                crouching_persons.append(crouching_person_info)
                
                logger.info(f"🔍 发现蹲着的人: 置信度={score:.3f}, 中心点=({center_x}, {center_y}), "
                           f"尺寸={width}x{height}, 长宽比={aspect_ratio:.2f}")
    
    return crouching_persons

def get_robot_direction_command(crouching_persons, image_width, dead_zone_ratio=0.3):
    """
    根据蹲着的人的位置生成机器人移动指令
    
    Args:
        crouching_persons: 蹲着的人的信息列表
        image_width: 图像宽度
        dead_zone_ratio: 中心死区比例（0-1），在此范围内不移动
    
    Returns:
        direction: 'left', 'right', 'stop', 'multiple_targets'
    """
    if not crouching_persons:
        return 'stop'  # 没有检测到蹲着的人
    
    if len(crouching_persons) > 1:
        # 多个目标时，选择置信度最高的
        crouching_persons = [max(crouching_persons, key=lambda x: x['score'])]
        logger.info("🎯 检测到多个目标，选择置信度最高的")
    
    # 获取目标的中心x坐标
    target = crouching_persons[0]
    target_center_x = target['center'][0]
    
    # 计算图像中心和死区范围
    image_center = image_width // 2
    dead_zone_width = image_width * dead_zone_ratio / 2
    
    left_boundary = image_center - dead_zone_width
    right_boundary = image_center + dead_zone_width
    
    if target_center_x < left_boundary:
        direction = 'left'
        logger.info(f"🤖 机器人指令: 向左移动 (目标在 x={target_center_x}, 左边界={left_boundary:.0f})")
    elif target_center_x > right_boundary:
        direction = 'right'
        logger.info(f"🤖 机器人指令: 向右移动 (目标在 x={target_center_x}, 右边界={right_boundary:.0f})")
    else:
        direction = 'stop'
        logger.info(f"🤖 机器人指令: 停止移动 (目标在中心区域 x={target_center_x})")
    
    return direction

def draw_detection(img: np.array, 
                   bbox: tuple[int, int, int, int],
                   score: float, 
                   class_id: int) -> None:
    """
    Draws a detection bounding box and label on the image.

    Parameters:
        img (np.array): The input image.
        bbox (tuple[int, int, int, int]): A tuple containing the bounding box coordinates (x1, y1, x2, y2).
        score (float): The detection score of the object.
        class_id (int): The class ID of the detected object.
    """
    x1, y1, x2, y2 = bbox
    color = rdk_colors[class_id%20]
    cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)
    label = f"{coco_names[class_id]}: {score:.2f}"
    (label_width, label_height), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)
    label_x, label_y = x1, y1 - 10 if y1 - 10 > label_height else y1 + 10
    cv2.rectangle(
        img, (label_x, label_y - label_height), (label_x + label_width, label_y + label_height), color, cv2.FILLED
    )
    cv2.putText(img, label, (label_x, label_y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)

def detect_once(frame, model, opt):
    """
    实时相机检测蹲着的人
    
    Args:
        model: YOLOv8检测模型实例
        opt: 命令行参数
    """
    logger.info("按 'q' 键退出，按 's' 键保存当前帧，按 'i' 键查看相机信息")
    
    # 检测处理
    input_tensor = model.preprocess_yuv420sp(frame.copy())
    outputs = model.c2numpy(model.forward(input_tensor))
    results = model.postProcess(outputs)
    
    # 蹲着的人检测
    crouching_persons = detect_crouching_persons(results, opt.crouch_ratio, opt.min_crouch_conf)
    
    # 生成机器人移动指令
    direction = get_robot_direction_command(crouching_persons, frame.shape[1])
    
    # 在帧上绘制检测结果
    display_frame = frame.copy()
    
    # 绘制所有检测结果（半透明）
    for class_id, score, x1, y1, x2, y2 in results:
        draw_detection(display_frame, (x1, y1, x2, y2), score, class_id)
    
    # 特别标记蹲着的人
    if crouching_persons:
        for person_info in crouching_persons:
            x1, y1, x2, y2 = person_info['bbox']
            center_x, center_y = person_info['center']
            
            # 红色边框标记蹲着的人
            cv2.rectangle(display_frame, (x1, y1), (x2, y2), (0, 0, 255), 4)
            # 黄色中心点
            cv2.circle(display_frame, (center_x, center_y), 8, (0, 255, 255), -1)
            # 蹲着的人标签
            crouch_label = f"CROUCHING: {person_info['score']:.2f} AR:{person_info['aspect_ratio']:.2f}"
            cv2.putText(display_frame, crouch_label, (x1, y1-35), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
    
    # 显示信息
    info_y = 30
    line_height = 35
    
    # 检测结果信息
    info_y += line_height
    if crouching_persons:
        count_text = f"Crouching Persons: {len(crouching_persons)}"
        cv2.putText(display_frame, count_text, (10, info_y), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
    else:
        cv2.putText(display_frame, "No Crouching Person", (10, info_y), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)
    
    # 机器人指令信息
    info_y += line_height
    direction_color = (0, 255, 0) if direction != 'stop' else (0, 255, 255)
    
    direction_text = f"Robot Command: {direction.upper()}"
    cv2.putText(display_frame, direction_text, (10, info_y), cv2.FONT_HERSHEY_SIMPLEX, 0.8, direction_color, 2)
    
    # 绘制中心死区
    image_center = frame.shape[1] // 2
    dead_zone_width = frame.shape[1] * 0.3 / 2
    left_boundary = int(image_center - dead_zone_width)
    right_boundary = int(image_center + dead_zone_width)
    
    # 绘制死区边界线
    cv2.line(display_frame, (left_boundary, 0), (left_boundary, frame.shape[0]), (255, 255, 0), 2)
    cv2.line(display_frame, (right_boundary, 0), (right_boundary, frame.shape[0]), (255, 255, 0), 2)
    cv2.line(display_frame, (image_center, 0), (image_center, frame.shape[0]), (255, 255, 255), 1)
    
    # 添加操作提示
    help_text = "Press 'q' to quit, 's' to save frame, 'i' for camera info"
    cv2.putText(display_frame, help_text, (10, frame.shape[0]-20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)

    return direction, display_frame